{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\Internship Progress\\\\Final all 3 datasets.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3ef24190e6ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mvaderSentiment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvaderSentiment\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSentimentIntensityAnalyzer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"D:\\\\Internship Progress\\\\Final all 3 datasets.xlsx\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m                 )\n\u001b[0;32m    295\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 304\u001b[1;33m         \u001b[0mio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    305\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m         raise ValueError(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path_or_buffer, engine)\u001b[0m\n\u001b[0;32m    865\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_io\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstringify_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engines\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    868\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    869\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_xlrd.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0merr_msg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Install xlrd >= 1.0.0 for Excel support\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mimport_optional_dependency\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"xlrd\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[0;32m    351\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_xlrd.py\u001b[0m in \u001b[0;36mload_workbook\u001b[1;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mopen_workbook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mopen_workbook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\xlrd\\__init__.py\u001b[0m in \u001b[0;36mopen_workbook\u001b[1;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m             \u001b[0mpeek\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpeek\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34mb\"PK\\x03\\x04\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# a ZIP file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\Internship Progress\\\\Final all 3 datasets.xlsx'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "data = pd.read_excel(\"D:\\\\Internship Progress\\\\Final all 3 datasets.xlsx\")\n",
    "print(data.head())\n",
    "text_column = 'snippet'\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def classify_sentiment(text):\n",
    "    sentiment_score = analyzer.polarity_scores(text)\n",
    "    compound_score = sentiment_score['compound']\n",
    "\n",
    "    if compound_score >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif compound_score <= -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "data['sentiment'] = data[text_column].apply(classify_sentiment)\n",
    "\n",
    "data.to_excel(\"D:\\Internship Progress\\Final all 3 datasetSENTIMENTS.xlsx\", index=False)\n",
    "\n",
    "print(data)\n",
    "\n",
    "count_positive = 0\n",
    "count_negative = 0\n",
    "count_neutral = 0\n",
    "\n",
    "for sentiment in data['sentiment']:\n",
    "    if sentiment == 'Positive':\n",
    "        count_positive += 1\n",
    "    elif sentiment == 'Negative':\n",
    "        count_negative += 1\n",
    "    elif sentiment == 'Neutral':\n",
    "        count_neutral += 1\n",
    "\n",
    "print(f\"Number of Positive Sentiments: {count_positive}\")\n",
    "print(f\"Number of Negative Sentiments: {count_negative}\")\n",
    "print(f\"Number of Neutral Sentiments: {count_neutral}\")\n",
    "\n",
    "per1 = 0\n",
    "per2 = 0\n",
    "per3 = 0\n",
    "total = len(data)\n",
    "postitive_response = (count_positive / total) * 100\n",
    "negative_response = (count_negative / total) * 100\n",
    "neutral_response = (count_neutral / total) * 100\n",
    "\n",
    "print(\"postitive_response\", postitive_response)\n",
    "print(\"negative_response\", negative_response)\n",
    "print(\"neutral_response\", neutral_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              snippet  \\\n",
      "0   Bravo Mumbai Immigration &Customs! They've rem...   \n",
      "1   - Application form - Cover letter with itinera...   \n",
      "2   3AM immigration queues at Bangalore Airport. I...   \n",
      "3   Myy son studying in the US landed at Delhi air...   \n",
      "4   Bangalore airport experience was stellar - tru...   \n",
      "..                                                ...   \n",
      "95  ... International Airport). More than that, it...   \n",
      "96  Parsis are Irani Migrants who landed & settled...   \n",
      "97  I booked my flight for Bengaluru but I asked t...   \n",
      "98  Jan 10, 2020 — Dear Mohit, We would like to in...   \n",
      "99  I'm seeking your help regarding my father's pa...   \n",
      "\n",
      "                                            highlighs sentiment  \n",
      "0                 ['Immigration', 'forms', 'airport']  Negative  \n",
      "1   ['Application form', 'flight', 'entry', 'airpo...  Negative  \n",
      "2        ['immigration', 'Bangalore Airport', 'form']   Neutral  \n",
      "3             ['airport', 'Bangalore', 'immigration']  Negative  \n",
      "4                ['Bangalore airport', 'immigration']  Negative  \n",
      "..                                                ...       ...  \n",
      "95  ['International Airport', 'arrivals', 'passport']  Negative  \n",
      "96                                       ['Migrants']   Neutral  \n",
      "97                                     ['no problem']  Positive  \n",
      "98  ['you can carry upto 2 litres of alcohol purch...  Positive  \n",
      "99   ['passport', 'airport immigration', 'departure']  Positive  \n",
      "\n",
      "[100 rows x 3 columns]\n",
      "Number of Positive Sentiments: 32\n",
      "Number of Negative Sentiments: 47\n",
      "Number of Neutral Sentiments: 21\n",
      "postitive_response 32.0\n",
      "negative_response 47.0\n",
      "neutral_response 21.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "data = pd.read_excel(\"D:\\Internship Progress\\immigration_cleaned.xlsx\")\n",
    "text_column = 'snippet'\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def classify_sentiment(text):\n",
    "    sentiment_score = analyzer.polarity_scores(text)\n",
    "    compound_score = sentiment_score['compound']\n",
    "\n",
    "    if compound_score >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif compound_score <= -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "data['sentiment'] = data[text_column].apply(classify_sentiment)\n",
    "\n",
    "data.to_excel(\"D:\\\\Internship Progress\\\\Final all 3 datasetSENTIMENTS2.xlsx\", index=False)\n",
    "\n",
    "print(data)\n",
    "\n",
    "count_positive = 0\n",
    "count_negative = 0\n",
    "count_neutral = 0\n",
    "\n",
    "for sentiment in data['sentiment']:\n",
    "    if sentiment == 'Positive':\n",
    "        count_positive += 1\n",
    "    elif sentiment == 'Negative':\n",
    "        count_negative += 1\n",
    "    elif sentiment == 'Neutral':\n",
    "        count_neutral += 1\n",
    "\n",
    "print(f\"Number of Positive Sentiments: {count_positive}\")\n",
    "print(f\"Number of Negative Sentiments: {count_negative}\")\n",
    "print(f\"Number of Neutral Sentiments: {count_neutral}\")\n",
    "\n",
    "per1 = 0\n",
    "per2 = 0\n",
    "per3 = 0\n",
    "total = len(data)\n",
    "postitive_response = (count_positive / total) * 100\n",
    "negative_response = (count_negative / total) * 100\n",
    "neutral_response = (count_neutral / total) * 100\n",
    "\n",
    "print(\"postitive_response\", postitive_response)\n",
    "print(\"negative_response\", negative_response)\n",
    "print(\"neutral_response\", neutral_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              snippet  \\\n",
      "0   May 22, 2019 — Dive into the luxury and unmatc...   \n",
      "1   Taj Bangalore is a mere walk away from the Kem...   \n",
      "2   Sep 14, 2018 — We're offering a full range of ...   \n",
      "3   Red Key is a budget-friendly 2-star hotel in B...   \n",
      "4   Sep 14, 2018 — We're offering a full range of ...   \n",
      "..                                                ...   \n",
      "94  for making this tour happen The security was t...   \n",
      "95  Thank you for your wonderful hospitality and s...   \n",
      "96  Free event invite on Amex Platinum card at ITC...   \n",
      "97  YCO contestants getting ready for cook off at ...   \n",
      "98  for your wonderful hospitality in Germany.    ...   \n",
      "\n",
      "                                            highlighs sentiment  \n",
      "0   ['hospitality', 'Bangalore', 'Kempegowda Inter...  Negative  \n",
      "1   ['Bangalore', 'International Airport', 'hotel'...  Negative  \n",
      "2                          ['Hospitality', 'airport']   Neutral  \n",
      "3    ['hotel in Bangalore', 'Airport', 'hospitality']  Positive  \n",
      "4                          ['Hospitality', 'airport']   Neutral  \n",
      "..                                                ...       ...  \n",
      "94                                    ['hospitality']  Positive  \n",
      "95               ['hospitality', 'Bengaluru airport']  Positive  \n",
      "96                 ['hotels in Bengaluru', 'airport']  Positive  \n",
      "97                           ['Bangalore', 'airport']  Positive  \n",
      "98                       ['hospitality', 'Bangalore']  Positive  \n",
      "\n",
      "[99 rows x 3 columns]\n",
      "Number of Positive Sentiments: 58\n",
      "Number of Negative Sentiments: 14\n",
      "Number of Neutral Sentiments: 27\n",
      "postitive_response 58.58585858585859\n",
      "negative_response 14.14141414141414\n",
      "neutral_response 27.27272727272727\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "data = pd.read_excel(\"D:\\Internship Progress\\Hospitality_cleaned.xlsx\")\n",
    "text_column = 'snippet'\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def classify_sentiment(text):\n",
    "    sentiment_score = analyzer.polarity_scores(text)\n",
    "    compound_score = sentiment_score['compound']\n",
    "\n",
    "    if compound_score >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif compound_score <= -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "data['sentiment'] = data[text_column].apply(classify_sentiment)\n",
    "\n",
    "data.to_excel(\"D:\\\\Internship Progress\\\\Final all 3 datasetSENTIMENTS3.xlsx\", index=False)\n",
    "\n",
    "print(data)\n",
    "\n",
    "count_positive = 0\n",
    "count_negative = 0\n",
    "count_neutral = 0\n",
    "\n",
    "for sentiment in data['sentiment']:\n",
    "    if sentiment == 'Positive':\n",
    "        count_positive += 1\n",
    "    elif sentiment == 'Negative':\n",
    "        count_negative += 1\n",
    "    elif sentiment == 'Neutral':\n",
    "        count_neutral += 1\n",
    "\n",
    "print(f\"Number of Positive Sentiments: {count_positive}\")\n",
    "print(f\"Number of Negative Sentiments: {count_negative}\")\n",
    "print(f\"Number of Neutral Sentiments: {count_neutral}\")\n",
    "\n",
    "per1 = 0\n",
    "per2 = 0\n",
    "per3 = 0\n",
    "total = len(data)\n",
    "postitive_response = (count_positive / total) * 100\n",
    "negative_response = (count_negative / total) * 100\n",
    "neutral_response = (count_neutral / total) * 100\n",
    "\n",
    "print(\"postitive_response\", postitive_response)\n",
    "print(\"negative_response\", negative_response)\n",
    "print(\"neutral_response\", neutral_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "airport: 5.89533371218242\n",
      "bangalore: 5.394136358716413\n",
      "food: 4.823487634552764\n",
      "bengaluru: 2.7701025085776556\n",
      "international: 2.630780233823315\n",
      "india: 2.1741076509102384\n",
      "2023: 2.053593533435689\n",
      "image: 1.8536207437200516\n",
      "beverages: 1.728407148835584\n",
      "blr: 1.6604784776489478\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "a = pd.read_excel(\"D:\\Internship Progress\\Food and Beverages cleaned.xlsx\")\n",
    "data = pd.DataFrame(a)\n",
    "\n",
    "column_name = 'snippet'\n",
    "corpus = data[column_name].tolist()\n",
    "\n",
    "tokenized_data = [word_tokenize(text) for text in corpus]\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_data = [[word.lower() for word in text if word.isalnum() and word.lower() not in stop_words] for text in tokenized_data]\n",
    "\n",
    "preprocessed_data = [' '.join(text) for text in filtered_data]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(preprocessed_data)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_scores = tfidf_matrix.sum(axis=0).A1\n",
    "\n",
    "word_tfidf_pairs = [(word, score) for word, score in zip(feature_names, tfidf_scores)]\n",
    "\n",
    "word_tfidf_pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "top_keywords = word_tfidf_pairs[:10]\n",
    "for keyword, score in top_keywords:\n",
    "    print(f\"{keyword}: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\91831\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\91831\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\91831\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\91831\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: click in c:\\users\\91831\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\91831\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\91831\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\91831\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-rake in c:\\users\\91831\\anaconda3\\lib\\site-packages (1.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-rake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\91831\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\91831\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "\n",
    "a = pd.read_excel(\"D:\\\\Internship Progress\\\\Food and Beverages cleaned.xlsx\")\n",
    "data = pd.DataFrame(a)\n",
    "\n",
    "column_name = 'snippet'\n",
    "corpus = data[column_name]\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def extract_keywords(text):\n",
    "    words = word_tokenize(text)\n",
    "    words = [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]\n",
    "    freq_dist = FreqDist(words)\n",
    "    keywords = [word for word, freq in freq_dist.most_common(5)]\n",
    "    return keywords\n",
    "\n",
    "for i, text in enumerate(corpus):\n",
    "    keywords = extract_keywords(text)\n",
    "    data['Keywords'] = data[column_name].apply(extract_keywords)\n",
    "data\n",
    "data.to_excel(\"D:\\\\Internship Progress\\\\Food and Beverages cleaned.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\91831\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\91831\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "\n",
    "a = pd.read_excel(\"D:\\\\Internship Progress\\\\Hospitality_cleaned.xlsx\")\n",
    "data = pd.DataFrame(a)\n",
    "\n",
    "column_name = 'snippet'\n",
    "corpus = data[column_name]\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def extract_keywords(text):\n",
    "    words = word_tokenize(text)\n",
    "    words = [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]\n",
    "    freq_dist = FreqDist(words)\n",
    "    keywords = [word for word, freq in freq_dist.most_common(5)]\n",
    "    return keywords\n",
    "\n",
    "for i, text in enumerate(corpus):\n",
    "    keywords = extract_keywords(text)\n",
    "    data['Keywords'] = data[column_name].apply(extract_keywords)\n",
    "data\n",
    "data.to_excel(\"D:\\\\Internship Progress\\\\Hospitality_cleaned.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\91831\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\91831\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "\n",
    "a = pd.read_excel(\"D:\\\\Internship Progress\\\\immigration_cleaned.xlsx\")\n",
    "data = pd.DataFrame(a)\n",
    "\n",
    "column_name = 'snippet'\n",
    "corpus = data[column_name]\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def extract_keywords(text):\n",
    "    words = word_tokenize(text)\n",
    "    words = [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]\n",
    "    freq_dist = FreqDist(words)\n",
    "    keywords = [word for word, freq in freq_dist.most_common(5)]\n",
    "    return keywords\n",
    "\n",
    "for i, text in enumerate(corpus):\n",
    "    keywords = extract_keywords(text)\n",
    "    data['Keywords'] = data[column_name].apply(extract_keywords)\n",
    "data\n",
    "data.to_excel(\"D:\\\\Internship Progress\\\\immigration_cleaned.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
